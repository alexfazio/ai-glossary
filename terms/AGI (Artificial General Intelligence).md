---
share: true
---

The letters AGI stand for artificial **general** intelligence, a **hypothetical** type of AI that will be able to complete any intellectual task as well as humans can. [^7] AGI will make today’s most advanced AIs look like pocket calculators. [^8] The name AGI distinguishes the concept from the broader field of study of AI. It also makes it clear that true AI possesses intelligence that is both broad and adaptable. [^14] AI doomers think this could engender economic chaos or even a robot apocalypse. [^9]

In broad terms, AGI typically means artificial intelligence that matches (or outmatches) humans on a range of tasks. But specifics about what counts as human-like, what tasks, and how many all tend to get waved away: AGI is AI, but better. AGI, or [artificial general intelligence](https://www.technologyreview.com/2020/10/15/1010461/artificial-general-intelligence-robots-ai-agi-deepmind-google-openai/), is one of the hottest topics in tech today. It’s also one of the most controversial. A big part of the problem is that few people agree on what the term even means. [^3]

In the 1950s Alan Turing, a British mathematician, said that talking to a model that had achieved agi would be indistinguishable from talking to a human. Arguably the most advanced [large language models](https://www.economist.com/interactive/science-and-technology/2023/04/22/large-creative-ai-models-will-transform-how-we-live-and-work) already pass the Turing test. But in recent years tech leaders have moved the goalposts by suggesting a host of new definitions. [^8]

Three things stand out from the current visions for Artificial General Intelligence (AGI): a human-like ability to generalise, a superhuman ability to self-improve at an exponential rate, and a significant amount of wishful thinking. When people discuss AGI, they typically refer to human-like abilities. This causes the targets of the search for AGI to constantly shift. Defining human-like abilities is challenging: What exactly do people mean when they refer to human-like artificial intelligence? Is it human-like in the way you and I are, or human-like in the way Lazarus Long is? [^5] 

Artificial general intelligence (AGI) – often referred to as "strong AI," "full AI," "human-level AI" or "general intelligent action" – represents a significant future leap in the field of artificial intelligence. Unlike [Narrow AI](../Narrow%20AI.md), which is tailored for specific tasks, such as [detecting product flaws](https://techcrunch.com/2024/03/12/axion-rays-ai-attempts-to-detect-product-flaws-to-prevent-recalls/), [summarizing the news](https://techcrunch.com/2024/02/29/former-twitter-engineers-are-building-particle-an-ai-powered-news-reader/), or [building you a website](https://techcrunch.com/2024/02/22/10web-armenia/), AGI will be able to perform a broad spectrum of cognitive tasks at or above human levels. [^1]

## Ethical Concerns

The concept raises existential questions about humanity's role in and control of a future where machines can outthink, outlearn and outperform humans in virtually every domain. [^1]

The core of this concern lies in the unpredictability of AGI's decision-making processes and objectives, which might not align with human values or priorities (a concept [explored in-depth in science fiction since at least the 1940s](https://en.wikipedia.org/wiki/Three_Laws_of_Robotics)). There's concern that once AGI reaches a certain level of autonomy and capability, it might become impossible to contain or control, leading to scenarios where its actions cannot be predicted or reversed. [^1] 

At the extreme, the so-called "doomers" argue there is a real risk of AGI emerging spontaneously from current research and that this could be a threat to humanity. They call for urgent government action. Some of this comes from self-interested companies seeking
barriers to competition ("This is very dangerous and we are building it as fast as possible, but don't let anyone else do it"), but plenty of it is sincere. [^12]

However, for every expert who thinks AGI might be close, there's another who doesn't. There are some who think [LLMs](../LLM%20(Large%20Language%20Models).md) might scale all the way to AGI, and others who think we still need an unknown number of unknown further breakthroughs. More importantly, they would all agree that we don't actually know. [^13]

OpenAI, the research organization behind ChatGPT, has acknowledged that the development of Artificial General Intelligence (AGI) and superintelligence could potentially replace human labor. According to their website, AGI is defined as a system that surpasses human performance in most economically valuable work. And this is just AGI, not even considering superintelligence ([ASI](../ASI%20(Artificial%20Superintelligence).md)). [^2]

AGI further highlights questions around the nature of intelligence, the need for ethics in AI and the future relationship between humans and machines. [^6]

Such “artificial general intelligence” (agi) is, for some researchers, a kind of holy grail. Some think agi is within reach, and can be achieved simply by building ever-bigger [LLMs](../LLM%20(Large%20Language%20Models).md); others, like Dr [Yann Lecunn](../Yann%20Lecunn.md), disagree. [^10]

While the latest advances in [LLMs](../LLM%20(Large%20Language%20Models).md) such as Claude 3 continue to amaze, hardly anyone believes that AGI has yet been achieved. Of course, there is no consensus definition of what AGI is. OpenAI [defines](https://openai.com/our-structure) this as “a highly autonomous system that outperforms humans at most economically valuable work.” GPT-4 (or Claude Opus) certainly is not autonomous, nor does it clearly outperform humans for most economically valuable work cases. [^6]

AI expert Gary Marcus [offered](https://garymarcus.substack.com/p/dear-elon-musk-here-are-five-things) this AGI definition: “A shorthand for any intelligence … that is flexible and general, with resourcefulness and reliability comparable to (or beyond) human intelligence.” If nothing else, the [hallucinations](../Hallucinations.md) that still plague today’s LLM systems would not qualify as being dependable. [^6]

AGI requires systems that can understand and learn from their environments in a generalised way, have self-awareness and apply reasoning across diverse domains. While LLM models like Claude excel in specific tasks, AGI needs a level of flexibility, adaptability and understanding that it and other current models have not yet achieved. [^6]

## Predictions

Based on deep learning, it might never be possible for [LLMs](../LLM%20(Large%20Language%20Models).md) to ever achieve AGI. That is the view from researchers at Rand, who [state](https://www.rand.org/pubs/commentary/2024/02/why-artificial-general-intelligence-lies-beyond-deep.html) that these systems “may fail when faced with unforeseen challenges (such as optimized just-in-time supply systems in the face of COVID-19).” They conclude in a VentureBeat [article](https://venturebeat.com/ai/why-artificial-general-intelligence-lies-beyond-deep-learning/) that deep learning has been successful in many applications, but has drawbacks for realizing AGI. [^6]

[Ben Goertzel](../Ben%20Goertzel.md), a computer scientist and CEO of [Singularity NET](../Singularity%20NET.md), [opined](https://www.livescience.com/technology/artificial-intelligence/ai-agi-singularity-in-2027-artificial-super-intelligence-sooner-than-we-think-ben-goertzel) at the recent Beneficial AGI Summit that AGI is within reach, perhaps as early as 2027. This timeline is consistent with statements from Nvidia CEO Jensen Huang who [said](https://www.thestreet.com/technology/nvidia-ceo-jensen-huang-artificial-general-intelligence) AGI could be achieved within 5 years, depending on the exact definition. [^6]
  
In a a book published November 23, 2018 titled [_Architects of Intelligence_](http://book.mfordfuture.com/), writer and futurist Martin Ford interviewed 23 of the most prominent men and women who are working in AI today, including DeepMind CEO Demis Hassabis, Google AI Chief Jeff Dean, and Stanford AI director Fei-Fei Li. In an informal survey, Ford asked each of them to guess by which year there will be at least a 50 percent chance of AGI being built.

Of the 23 people Ford interviewed, only 18 answered, and of those, only two went on the record. Interestingly, those two individuals provided the most extreme answers: Ray Kurzweil, a futurist and director of engineering at Google, suggested that by 2029, there would be a 50 percent chance of AGI being built, and Rodney Brooks, roboticist and co-founder of iRobot, went for 2200. The rest of the guesses were scattered between these two extremes, with the average estimate being 2099 — 81 years from now.

In other words: AGI is a comfortable distance away, though you might live to see it happen.



### Footnotes

[^1]: https://techcrunch.com/2024/03/19/agi-and-hallucinations/
[^2]: https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/
[^3]: https://www.technologyreview.com/2023/11/16/1083498/google-deepmind-what-is-artificial-general-intelligence-agi/
[^5]: https://www.technologyreview.com/2020/10/15/1010461/artificial-general-intelligence-robots-ai-agi-deepmind-google-openai/
[^6]: https://venturebeat.com/ai/beyond-human-intelligence-claude-3-0-and-the-quest-for-agi/
[^7]: https://www.economist.com/1843/2023/09/29/who-moved-my-chips-life-in-an-ai-entrepreneurs-houseshare
[^8]: https://www.economist.com/1843/2019/03/01/deepmind-and-google-the-battle-to-control-artificial-intelligencehttps://www.economist.com/the-economist-explains/2024/03/28/how-to-define-artificial-general-intelligence
[^9]: https://www.economist.com/business/2024/01/17/the-bosses-of-openai-and-microsoft-talk-to-the-economist
[^10]: https://www.economist.com/science-and-technology/2023/04/19/large-language-models-ability-to-generate-text-also-lets-them-plan-and-reason
[^11]: https://www.ft.com/content/774901e5-e831-4e0b-b0a1-e4b5b0032fb8

https://www.ft.com/content/1d1cb2b3-391c-4dc8-ba5b-fedd379b7fb0
[^12]: https://www.ft.com/content/4cecce94-48a6-4eba-b914-dd23d1e11ac9
[^13]: https://www.ft.com/content/4cecce94-48a6-4eba-b914-dd23d1e11ac9
[^14]: https://www.theverge.com/2018/11/27/18114362/ai-artificial-general-intelligence-when-achieved-martin-ford-book