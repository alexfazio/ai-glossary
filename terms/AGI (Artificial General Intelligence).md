---
share: true
---

The letters AGI stand for artificial general intelligence, a **hypothetical** type of AI that will be able to complete any intellectual task as well as humans can. [^7] AGI will make today’s most advanced AIs look like pocket calculators. [^8] AI doomers think this could engender economic chaos or even a robot apocalypse. [^9]

In broad terms, AGI typically means artificial intelligence that matches (or outmatches) humans on a range of tasks. But specifics about what counts as human-like, what tasks, and how many all tend to get waved away: AGI is AI, but better. AGI, or [artificial general intelligence](https://www.technologyreview.com/2020/10/15/1010461/artificial-general-intelligence-robots-ai-agi-deepmind-google-openai/), is one of the hottest topics in tech today. It’s also one of the most controversial. A big part of the problem is that few people agree on what the term even means. [^3]

In the 1950s Alan Turing, a British mathematician, said that talking to a model that had achieved agi would be indistinguishable from talking to a human. Arguably the most advanced [large language models](https://www.economist.com/interactive/science-and-technology/2023/04/22/large-creative-ai-models-will-transform-how-we-live-and-work) already pass the Turing test. But in recent years tech leaders have moved the goalposts by suggesting a host of new definitions. [^8]

Three things stand out from the current visions for Artificial General Intelligence (AGI): a human-like ability to generalise, a superhuman ability to self-improve at an exponential rate, and a significant amount of wishful thinking. When people discuss AGI, they typically refer to human-like abilities. This causes the targets of the search for AGI to constantly shift. Defining human-like abilities is challenging: What exactly do people mean when they refer to human-like artificial intelligence? Is it human-like in the way you and I are, or human-like in the way Lazarus Long is? [^5] 

Artificial general intelligence (AGI) – often referred to as "strong AI," "full AI," "human-level AI" or "general intelligent action" – represents a significant future leap in the field of artificial intelligence. Unlike [Narrow AI](../Narrow%20AI.md), which is tailored for specific tasks, such as [detecting product flaws](https://techcrunch.com/2024/03/12/axion-rays-ai-attempts-to-detect-product-flaws-to-prevent-recalls/), [summarizing the news](https://techcrunch.com/2024/02/29/former-twitter-engineers-are-building-particle-an-ai-powered-news-reader/), or [building you a website](https://techcrunch.com/2024/02/22/10web-armenia/), AGI will be able to perform a broad spectrum of cognitive tasks at or above human levels. [^1]

The concept raises existential questions about humanity's role in and control of a future where machines can outthink, outlearn and outperform humans in virtually every domain. [^1]

The core of this concern lies in the unpredictability of AGI's decision-making processes and objectives, which might not align with human values or priorities (a concept [explored in-depth in science fiction since at least the 1940s](https://en.wikipedia.org/wiki/Three_Laws_of_Robotics)). There's concern that once AGI reaches a certain level of autonomy and capability, it might become impossible to contain or control, leading to scenarios where its actions cannot be predicted or reversed. [^1]

OpenAI, the research organization behind ChatGPT, has acknowledged that the development of Artificial General Intelligence (AGI) and superintelligence could potentially replace human labor. According to their website, AGI is defined as a system that surpasses human performance in most economically valuable work. And this is just AGI, not even considering superintelligence ([ASI](../ASI%20(Artificial%20Superintelligence).md)). [^2]

AGI further highlights questions around the nature of intelligence, the need for ethics in AI and the future relationship between humans and machines. [^6]

Such “artificial general intelligence” (agi) is, for some researchers, a kind of holy grail. Some think agi is within reach, and can be achieved simply by building ever-bigger [LLM](../LLM.md); others, like Dr [Yann Lecunn](../Yann%20Lecunn.md), disagree. [^10]

While the latest advances in LLMs such as Claude 3 continue to amaze, hardly anyone believes that AGI has yet been achieved. Of course, there is no consensus definition of what AGI is. OpenAI [defines](https://openai.com/our-structure) this as “a highly autonomous system that outperforms humans at most economically valuable work.” GPT-4 (or Claude Opus) certainly is not autonomous, nor does it clearly outperform humans for most economically valuable work cases. [^6]

AI expert Gary Marcus [offered](https://garymarcus.substack.com/p/dear-elon-musk-here-are-five-things) this AGI definition: “A shorthand for any intelligence … that is flexible and general, with resourcefulness and reliability comparable to (or beyond) human intelligence.” If nothing else, the hallucinations that still plague today’s LLM systems would not qualify as being dependable. [^6]

AGI requires systems that can understand and learn from their environments in a generalized way, have self-awareness and apply reasoning across diverse domains. While LLM models like Claude excel in specific tasks, AGI needs a level of flexibility, adaptability and understanding that it and other current models have not yet achieved. [^6]

Based on deep learning, it might never be possible for LLMs to ever achieve AGI. That is the view from researchers at Rand, who [state](https://www.rand.org/pubs/commentary/2024/02/why-artificial-general-intelligence-lies-beyond-deep.html) that these systems “may fail when faced with unforeseen challenges (such as optimized just-in-time supply systems in the face of COVID-19).” They conclude in a VentureBeat [article](https://venturebeat.com/ai/why-artificial-general-intelligence-lies-beyond-deep-learning/) that deep learning has been successful in many applications, but has drawbacks for realizing AGI. [^6]

Ben Goertzel, a computer scientist and CEO of Singularity NET, [opined](https://www.livescience.com/technology/artificial-intelligence/ai-agi-singularity-in-2027-artificial-super-intelligence-sooner-than-we-think-ben-goertzel) at the recent Beneficial AGI Summit that AGI is within reach, perhaps as early as 2027. This timeline is consistent with statements from Nvidia CEO Jensen Huang who [said](https://www.thestreet.com/technology/nvidia-ceo-jensen-huang-artificial-general-intelligence) AGI could be achieved within 5 years, depending on the exact definition. [^6]

### Footnotes

[^1]: https://techcrunch.com/2024/03/19/agi-and-hallucinations/
[^2]: https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/
[^3]: https://www.technologyreview.com/2023/11/16/1083498/google-deepmind-what-is-artificial-general-intelligence-agi/
[^5]: https://www.technologyreview.com/2020/10/15/1010461/artificial-general-intelligence-robots-ai-agi-deepmind-google-openai/
[^6]: https://venturebeat.com/ai/beyond-human-intelligence-claude-3-0-and-the-quest-for-agi/
[^7]: https://www.economist.com/1843/2023/09/29/who-moved-my-chips-life-in-an-ai-entrepreneurs-houseshare
[^8]: https://www.economist.com/1843/2019/03/01/deepmind-and-google-the-battle-to-control-artificial-intelligencehttps://www.economist.com/the-economist-explains/2024/03/28/how-to-define-artificial-general-intelligence
[^9]: https://www.economist.com/business/2024/01/17/the-bosses-of-openai-and-microsoft-talk-to-the-economist
[^10]: https://www.economist.com/science-and-technology/2023/04/19/large-language-models-ability-to-generate-text-also-lets-them-plan-and-reason