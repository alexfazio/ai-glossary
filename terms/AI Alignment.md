---
share: true
---
An aligned AI is an AI that is in harmony with human values. A _misaligned_ AI system may pursue some objectives, but not the intended ones. [^3]

In short: How do we ensure that the AI we build, which might very well be significantly smarter than any person who has ever lived, is aligned with the interests of its creators and of the human race? An unaligned superintelligent AI ([ASI](../ASI%20(Artificial%20Superintelligence).md)) could be quite a problem. [^2]

AI Alignment is the technical term for a concern about artificial intelligence that was first raised by Wiener in his 1960 essay, "God & Golem, Inc." Wiener, known as the father of cybernetics, expressed worry about a future where machines could learn and develop unexpected strategies at a pace that confounds their programmers. He speculated that these strategies might involve actions that the programmers did not truly intend, but were instead just colorful imitations of their intentions. [^Source]

To illustrate his point, Wiener referenced the fable "The Sorcerer's Apprentice" by the German poet Goethe. In this story, an apprentice magician enchants a broom to fetch water for his master's bath. However, the apprentice is unable to stop the broom once its task is complete. The broom continues to fetch water until it floods the room, demonstrating a lack of common sense to know when to stop. [^Source]

Wiener's concern is that, similar to Goethe's enchanted broom, an AI might single-mindedly pursue a goal set by a user, but in the process, it could do something harmful that was not intended. [^Source]

The most recognised example of misalignment is the "paperclip maximiser," [^1] a thought experiment introduced by philosopher Nick Bostrom in 2003. In this scenario, an artificial intelligence is tasked with producing as many paperclips as possible. Due to its limited understanding, such an open-ended objective prompts the maximiser to take any necessary steps, even if it means covering the Earth with paperclip factories and eliminating humanity in the process.
### Footnotes

[^1]: https://en.m.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer
[^Source]: https://www.economist.com/science-and-technology/2023/04/19/how-generative-models-could-go-wrong
[^3]: https://www.pearson.com/en-us/subject-catalog/p/artificial-intelligence-a-modern-approach/P200000003500?view=educator
[^2]: https://www.theatlantic.com/newsletters/archive/2023/02/ai-chatgpt-microsoft-bing-chatbot-questions/673202/