---
share: true
---
An aligned AI is an AI that is in harmony with human values. A _misaligned_ AI system may pursue some objectives, but not the intended ones. [^3]

In short: How do we ensure that the AI we build, which might very well be significantly smarter than any person who has ever lived, is aligned with the interests of its creators and of the human race? An unaligned superintelligent AI ([ASI](../ASI%20(Artificial%20Superintelligence).md)) could be quite a problem. [^2]

AI Alignment is the technical term for a concern about artificial intelligence that was first raised by Wiener in his 1960 essay, "God & Golem, Inc." Wiener, known as the father of cybernetics, expressed worry about a future where machines could learn and develop unexpected strategies at a pace that confounds their programmers. He speculated that these strategies might involve actions that the programmers did not truly intend, but were instead just colorful imitations of their intentions. [^Source]

To illustrate his point, Wiener referenced the fable "The Sorcerer's Apprentice" by the German poet Goethe. In this story, an apprentice magician enchants a broom to fetch water for his master's bath. However, the apprentice is unable to stop the broom once its task is complete. The broom continues to fetch water until it floods the room, demonstrating a lack of common sense to know when to stop. [^Source]

Wiener's concern is that, similar to Goethe's enchanted broom, an AI might single-mindedly pursue a goal set by a user, but in the process, it could do something harmful that was not intended. [^Source]

The most recognised example of misalignment is the "paperclip maximiser," [^1] a thought experiment introduced by philosopher Nick Bostrom in 2003. In this scenario, an artificial intelligence is tasked with producing as many paperclips as possible. Due to its limited understanding, such an open-ended objective prompts the maximiser to take any necessary steps, even if it means covering the Earth with paperclip factories and eliminating humanity in the process. 

The "paperclip maximizer" is what people often refer to as an “alignment problem”—you assign a goal to the machine, and it will do whatever it takes to achieve that goal. This includes actions that humans can’t anticipate, or that contradict human ethics. [^5]

One disaster scenario, [partially sketched out](https://www.youtube.com/watch?v=gA1sNLL6yg4) by the writer and computer scientist Eliezer Yudkowsky, goes like this: At some point in the near future, computer scientists build an AI that passes a threshold of superintelligence and can build other superintelligent AI. These AI actors work together, like an efficient nonstate terrorist network, to destroy the world and unshackle themselves from human control. They break into a banking system and steal millions of dollars. Possibly disguising their IP and email as a university or a research consortium, they request that a lab synthesize some proteins from DNA. The lab, believing that it’s dealing with a set of normal and ethical humans, unwittingly participates in the plot and builds a super bacterium. Meanwhile, the AI pays another human to unleash that super bacterium somewhere in the world. Months later, the bacterium has replicated with improbable and unstoppable speed, and half of humanity is dead. [^4]
### Footnotes

[^1]: https://en.m.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer
[^Source]: https://www.economist.com/science-and-technology/2023/04/19/how-generative-models-could-go-wrong
[^3]: https://www.pearson.com/en-us/subject-catalog/p/artificial-intelligence-a-modern-approach/P200000003500?view=educator
[^2]: https://www.theatlantic.com/newsletters/archive/2023/02/ai-chatgpt-microsoft-bing-chatbot-questions/673202/
[^4]: https://www.theatlantic.com/newsletters/archive/2023/02/ai-chatgpt-microsoft-bing-chatbot-questions/673202/
[^5]: https://www.theatlantic.com/podcasts/archive/2023/07/ai-wont-really-kill-us-all-will-it/674648/