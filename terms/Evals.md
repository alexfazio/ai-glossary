---
share: true
---

Evals in LLM lingo refer to a framework for evaluating Large Language Models (LLMs) and LLM systems. It is an open-source framework used for assessing the performance and capabilities of these language models[^5].

Citations:
[^5] https://community.openai.com/t/gpt-4-has-been-severely-downgraded-topic-curation/304946

**Related**: 

### Citations