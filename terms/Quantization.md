---
share: true
---

LLMs can take gigabytes of memory to store, which limits what can be run on consumer hardware. But quantization can dramatically compress models, making a wider selection of models available to developers. You can often reduce model size by 4x or more while maintaining reasonable performance. [^1] 

As models get bigger and bigger, quantization becomes more important for making models practical and accessible. [^1] 

**Related**: 

### Citations

[^1]: https://x.com/AndrewYNg/status/1779905922602782752