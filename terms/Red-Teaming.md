---
share: true
---
Similar to a [penetration test](https://en.m.wikipedia.org/wiki/Penetration_test), Red Teaming is an adversarial examination of the AI system to ensure that AI systems are ethical and safe.

The "red team” would “qualitatively probe [and] adversarially test” new models, attempting to break them. [^1]

In the year 2023, [OpenAI](https://en.m.wikipedia.org/wiki/OpenAI) employed 50 academics and experts to examine the capabilities of the GPT-4 model, which currently powers the premium version of ChatGPT. This process is known as "red-teaming".  [^3]

Over a span of six months, this team, consisting of experts from various fields such as chemistry, nuclear weapons, law, education, and misinformation, was tasked to qualitatively probe and adversarially test the new model in an effort to identify its weaknesses. Red-teaming is a strategy also used by other organisations like Google DeepMind and Anthropic to identify and rectify the weaknesses in their software. [^2]

While RLHF and red-teaming are crucial for AI safety, they do not completely eliminate the issue of harmful AI outputs [^2]

### Footnotes

[^1]: https://www.ft.com/content/0876687a-f8b7-4b39-b513-5fee942831e8
[^2]: https://www.ft.com/content/f23e59a2-4cad-43a5-aed9-3aea6426d0f2
[^3]: https://www.ft.com/content/f23e59a2-4cad-43a5-aed9-3aea6426d0f2