---
share: true
---

Approximately 47 billion, but only 12 billion are actively used at any one time.
Configuration: Consists of eight experts, each with around 7 billion parameters.
Performance: Demonstrates superior performance in most real-world tasks compared to GPT-3.5 and the former leading open-source LLM, Llama 2 70b.
Mixture of Experts (MoE): A neural network architecture where specific layers are replaced with multiple smaller networks, or "experts," managed by a gate network or router. Key points include:

### Related Articles

### Citations
